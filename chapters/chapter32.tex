\chapter{Неравенство Чебышева и закон больших чисел. Предельная теорема Пуассона.}
\section{Неравенство Чебышева}

К уже доказанным свойствам математического ожидания добавим ещё одно.
\begin{defn}
С каждым событием $A \in \mathcal{A}$ свяжем дискретную случайную величину
$$
\chi_A =\chi_A(w)=\begin{cases}
1,&\text{если $w \in A$;}\\
0,&\text{если $w \notin A$;}
\end{cases}
$$
называемую \textit{индикатором события $A$} .
\end{defn}
\begin{thm}
Математическое ожидание индикатора $\chi_A$ события $A$ равно вероятности этого события.
$$
\bbE\chi_A=\bbP(A).
$$ 
\end{thm}
\begin{proof}
Так как $\bbP(A)=\sum\limits_{w\in A}p(w)$, то 
$$
\bbE\chi_A=\sum\limits_{w\in\Omega}\chi_A(w)p(w)=\sum\limits_{w\in A}1\cdot p(w)+\sum\limits_{w\notin A}0\cdot p(w)=\sum\limits_{w\in A}p(w)=\bbP(A).
$$
Это утверждение, конечно же, верно и в непрерывном случае, и в самом общем случае.
\end{proof}

\begin{thm}[неравенство Маркова\rindex{неравенство!Маркова}]
Пусть случайная величина $\xi$ принимает неотрицательные значения. Пусть $a>0$. Тогда 
$$
\bbP(\xi \ge a)\le\frac{\bbE\xi}{a}.
$$
\end{thm}
\begin{proof}
Представим $\xi$ в виде суммы двух неотрицательных случайных величин
$$
\xi = \xi\cdot\chi_{\{\xi\ge a\}}+\xi\cdot\chi_{\{\xi< a\}}.
$$
По свойствам аддитивности и монотонности имеем:
$$
\bbE \xi=\bbE (\xi\cdot\chi_{\{\xi\ge a\}})+\underbrace{\bbE (\xi\cdot\chi_{\{\xi< a\}})}_{\ge 0} \ge \bbE (\xi\cdot\chi_{\{\xi\ge a\}}).
$$
Так как $\xi\cdot\chi_{\{\xi\ge a\}}\ge a\cdot\chi_{\{\xi\ge a\}}$, то применяя ещё раз свойство монотонности, получаем
$$
\bbE \xi\ge a\bbE (\chi_{\{\xi\ge a\}})=a\cdot \bbP(\xi\ge a),
$$
что и доказывает неравенство Маркова.
\end{proof}

\begin{thm}[неравенство Чебышева\rindex{неравенство!Чебышева}]
Если случайная величина $\xi$ имеет дисперсию $\bbD \xi$, и пусть $b>0$, то
$$
\bbP(|\xi-\bbE \xi|\ge b ) \le\frac{\bbD \xi}{b^2}.
$$
\end{thm} 
\begin{proof}
Уже доказано неравенство Маркова $\bbP(\eta \ge a) \le \frac{\bbE\eta}{a}$  для $\eta \ge 0$, $a>0$. Полагая в нем $\eta = |\xi-\bbE \xi|^2$ и $a=b^2$, получаем неравенство Чебышева.
\end{proof}

\section{Закон больших чисел}

Говорят, что к случайным величинам $\xi_k$, $k \in \bbN$, имеющим математические ожидания $\bbE\xi_k$,  $k \in \bbN$, применим закон больших чисел, если для любого $\epsilon > 0$
\begin{equation} \label{ch32.2eq4}
\lim_{n \to \infty} \bbP \left( \left| \frac{\xi_1 + \xi_2 + \ldots + \xi_n}{n} - \frac{\bbE \xi_1 + \bbE \xi_2 + \ldots + \bbE \xi_n}{n} \right| < \epsilon \right) = 1
\end{equation}

\begin{thm} [Маркова]
Если у случайных величин $\xi_k$, $k \in \bbN$, существуют дисперсии и если при $n \to \infty$
\begin{equation} \label{ch32.2eq5}
\frac{1}{n^2} \bbD  \left( \sum_{k  = 1}^{n} \xi_k\right) \to 0,
\end{equation}
то к случайным величинам $\xi_k$, $k \in \bbN$, применим закон больших чисел.
\end{thm}

\begin{proof}
Обозначим $\eta_n = (\xi_1 + \ldots + \xi_n) / n$. Пользуясь неравенством Чебышева, в котором положим $b = \epsilon, \: \xi = \eta_n$, получим
$$
\bbP \left( \left| \eta_n - \bbE  \eta_n \right| \ge \epsilon \right) \le \frac{1}{\epsilon^2} \bbD  \eta_n.
$$

Отсюда, так как
$$
\bbE  \eta_n = \frac{\bbE \xi_1 + \ldots + \bbE \xi_n}{n}, \quad \bbD \eta_n = \frac{1}{n^2} \bbD  \left( \sum_{k  = 1}^{n} \xi_k\right),
$$ 
для вероятности противоположного события находим оценку
$$
\bbP \left( \left| \frac{\xi_1 + \ldots + \xi_n}{n} - \frac{\bbE \xi_1 + \ldots + \bbE \xi_n}{n} \right| < \epsilon \right) \ge 1 - \frac{1}{\epsilon^2n^2} \bbD  \left( \sum_{k  = 1}^{n} \xi_k\right).
$$

Из этого неравенства и условия \eqref{ch32.2eq5} следует \eqref{ch32.2eq4}.
\end{proof}

Некоторые частные случаи этой теоремы.

\begin{thm} [Чебышева]\label{ch32.2T3}
Если случайные величины $\xi_k, \: k = 1,2,\ldots$, попарно независимы, имеют равномерно ограниченные дисперсии (т.е. существует постоянная с такая, что $\bbD  \xi_k < c$ при всех $k = 1, 2\ldots$),	то к случайным величинам $\xi_k, \: k = 1,2,\ldots$, применим закон больших чисел.
\end{thm}

\begin{proof}
В самом деле, для доказательства теоремы достаточно проверить условие \eqref{ch32.2eq5}. Из неравенств $\bbD  \xi_k < c, \: k = 1, 2\ldots$, и попарной независимости случайных величин $\xi_k, \: k = 1,2,\ldots$, следует, что
$$
\bbD  \left( \sum_{k  = 1}^{n} \xi_k\right) = \sum_{k  = 1}^{n} \bbD  \xi_k  < nc.
$$

Отсюда получаем условие $\eqref{ch32.2eq5}$:
\begin{equation*}
\frac{1}{n^2} \bbD  \left( \sum_{k  = 1}^{n} \xi_k\right) < \frac{c}{n} \xrightarrow{n \to \infty} 0. \tag*{\qedhere}
\end{equation*}
\end{proof}

\begin{thm} \label{ch32.2T4}
Если случайные величины $\xi_k, \: k = 1,2,\ldots$, одинаково распределены, попарно независимы и имеют конечные дисперсии, то к этим случайным величинам применим закон больших чисел.
\end{thm}
\begin{proof}
Теорема \ref{ch32.2T4} следует из теоремы \ref{ch32.2T3}. Действительно, дисперсии $\bbD\xi_k, \: k = 1,2, \ldots,$ существуют и равны между собой; следовательно, они равномерно ограничены и мы находимся в условиях теоремы \ref{ch32.2T3}.
\end{proof}
Утверждение теоремы \ref{ch32.2T4} означает, что для любых $\epsilon > 0, \: \delta > 0$, найдётся такое $N$, что при $n > N$ верно неравенство
\begin{equation} \label{ch32.2eq6}
\bbP \left( \left| \frac{\xi_1 + \xi_2 + \dots + \xi_n}{n} - a \right| < \epsilon \right) \ge 1 - \delta,
\end{equation}
где $a = \bbE  \xi_k$,  $k \in \bbN$

\section{Схема Бернулли}

\textit{Схема независимых испытаний, в которой каждое испытание может закончиться только одним из двух исходов, называется схемой Бернулли.}

Обычно эти исходы называют <<успехом>> и <<неудачей>>, а их вероятности обозначают $p$ и $q = 1 - p$ $(0 \le p \le 1)$ соответственно.

Наступление или ненаступление события $A$ в испытаниях с разными номерами для схемы Бернулли независимы. Значит, в силу теоремы умножения вероятностей, вероятность того, что событие $A$ наступит в $m$ определённых испытаниях (например, в испытаниях с номерами $s_1, s_2, \ldots, s_m$), а при остальных $n - m$ не наступит, равна $p^mq^{n - m}$. Эта вероятность не зависит от  расположения номеров $s_1, s_2, \ldots, s_m$.

Простейшая задача, относящаяся к схеме Бернулли, состоит в определении вероятности $\bbP_n(m)$ того, что в $n$ испытаниях событие $A$ произойдёт $m$ раз ($0 \le m \le n$).

Мы только что нашли, что вероятность того, что событие $A$ наступит в испытаниях с определёнными $m$ номерами, а в остальных не наступит равна $p^mq^{n - m}$. По теореме сложения искомая вероятность равна сумме только что вычисленных вероятностей для всех различных способов размещения $m$ появлений события $A$ и $n - m$ непоявлений среди $n$ испытаний. Число таких способов известно из комбинаторики. Это число обозначается  $C_n^m$ или $\binom n m $. И оно равно	
$$
\binom n m = \frac{n!}{m!\;(n - m)!}
$$
и, следовательно,
\begin{equation} \label{ch32.1.1eq1}
\bbP_n(m) = \binom n m p^mq^{n - m} \quad (m = 0,1,2,\ldots,n).
\end{equation}

\begin{thm} [Бернулли\rindex{теорема!Бернулли}]
Пусть $\mu_n$ --- число успехов в $n$ испытаниях Бернулли и $p$ "--- вероятность успеха в каждом отдельном испытании. Тогда $\forall \epsilon > 0$
\begin{equation} \label{ch32.2eq7}
\lim_{n \to \infty} \bbP \left( \left| \frac{\mu_n}{n} - p \right| < \epsilon \right) = 1.
\end{equation}
\end{thm}

\begin{proof}
Для доказательства этой теоремы воспользуемся представлением $\mu_n$ в виде суммы $n$ индикаторов: $\mu_n = \xi_1 + \xi_2 + \ldots + \xi_n$, где $\xi_k = 1$, если в $k$-м испытании был успех, и $\xi_k  = 0$ в противном случае.

Так как $\xi_k,\; k = 1,2,\ldots, \: n$, независимы, одинаково распределены $(\bbP( \xi_k = 1) = p, \,\bbP( \xi_k = 0) = 1 - p = q)$, то дисперсии случайных величин $\xi_k$ существуют и $\bbE  \xi_k = p$, то теорема Бернулли сразу следует из теоремы $\ref{ch32.2T4}$.
\end{proof}

\section{Предельная теорема Пуассона}
\begin{thm} [Пуассона\rindex{теорема!Пуассона}]
Если $n \to \infty$ и $p \to 0$ так, что $np \to \lambda, \:\\ 0 < \lambda < \infty$, то
$$
\bbP(\mu_n = m) = \bbP_n(m) = \binom n m p^mq^{n - m} \to p_m(\lambda) = \frac{\lambda^m}{m!} e^{-\lambda}
$$
при любом постоянном $m, \: m = 0,1,2,\dots$.
\end{thm}

\begin{proof}
Положив $np = \lambda_n$, представим вероятность $\bbP_n(m)$ в виде
\begin{multline*}
\bbP(\mu_n = m) = \frac{n(n - 1)\ldots(n - m + 1)}{m!} \left( \frac{\lambda_n}{n}\right)^m\left( 1 - \frac{\lambda_n}{n}\right)^{n - m} = \\
= \frac{\lambda_n^m}{m!} \left( 1 - \frac{\lambda_n}{n}\right)^n \left( 1 - \frac{1}{n}\right)\left(1 - \frac{2}{n} \right) \ldots \left( 1 - \frac{m - 1}{n}\right)\left(1 - \frac{\lambda_n}{n} \right)^{-m}.
\end{multline*}

Отсюда при $n \to \infty$ получим утверждение теоремы.
\end{proof}

Таким образом, при больших $n$ и малых $p$ мы можем воспользоваться приближенной формулой
$$
\bbP(\mu_n = m) \approx \frac{(\lambda_n)^m}{m!} e^{-\lambda_n},	\lambda_n = np.
$$
